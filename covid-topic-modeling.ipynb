{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T04:36:28.525185Z",
     "start_time": "2020-07-01T04:36:24.608901Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import preprocess_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T04:38:23.105852Z",
     "start_time": "2020-07-01T04:36:28.530733Z"
    }
   },
   "outputs": [],
   "source": [
    "# read with spark because it's a lot faster\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, udf\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.worker.cleanup.enabled\", \"true\") \\\n",
    ".config(\"spark.worker.cleanup.interval\", 60) \\\n",
    ".getOrCreate() \n",
    "df = spark.read.json(\"parsed_tweets3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T04:38:23.515796Z",
     "start_time": "2020-07-01T04:38:23.122944Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "@udf(\"string\")\n",
    "def replace_entities(mentions, urls, tweet):\n",
    "    mentions = ['@' + m for m in mentions.split(' ')]\n",
    "    for m in mentions:\n",
    "        if len(m) > 0:\n",
    "            tweet = tweet.replace(m, '@MENTION')\n",
    "        \n",
    "    urls = urls.split(' ')\n",
    "    for u in urls:\n",
    "        if len(u) > 0:\n",
    "            tweet = tweet.replace(u, '@URL')\n",
    "    http = re.compile(r'https?://\\S+')\n",
    "    tweet = http.sub('@URL', tweet)\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T04:38:23.673554Z",
     "start_time": "2020-07-01T04:38:23.522056Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"entity_replaced\", replace_entities(df.mentions, df.urls, df.full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T04:38:24.080060Z",
     "start_time": "2020-07-01T04:38:23.677846Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.where(df.lang == \"en\")\n",
    "df = df.withColumn(\"rt_indicator\", when(df.full_text.like(\"RT @%\"), 1).otherwise(0))\n",
    "training_df = df.where(df.rt_indicator == 0).sample(0.15)\n",
    "full_df = df.sample(0.05)\n",
    "df = training_df.union(full_df).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T04:38:24.950252Z",
     "start_time": "2020-07-01T04:38:24.083692Z"
    }
   },
   "outputs": [],
   "source": [
    "from demoji import replace\n",
    "import re\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces, remove_stopwords\n",
    "from gensim.utils import to_unicode\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "STOPWORDS = STOPWORDS.union(stopwords.words('english')).union(set('&amp;'))\n",
    "\n",
    "def my_remove_stopwords(s):\n",
    "    s = to_unicode(s)\n",
    "    s = s.lower()\n",
    "    return \" \".join(w for w in s.split() if w not in STOPWORDS)\n",
    "\n",
    "def replacer(string):\n",
    "    string = string.replace(\"-\", \"\")\n",
    "    string = string.replace(\"RT \", \"\")\n",
    "    handles_and_hashtags = re.compile(r\"[^\\w\\d#@\\s]+\")\n",
    "    string =  handles_and_hashtags.sub('', string)\n",
    "    string = replace(string)\n",
    "    return string\n",
    "\n",
    "def ignore_stemmer(tweet):\n",
    "    stemmed_list = []\n",
    "    split = tweet.split(\" \")\n",
    "    for w in split:\n",
    "        if not w.startswith(\"@\") and not w.startswith(\"#\"):\n",
    "            w = SnowballStemmer(\"english\").stem(w)\n",
    "        stemmed_list.append(w)\n",
    "    return \" \".join(stemmed_list)\n",
    "\n",
    "def further_replacer(tweet):\n",
    "    split = tweet.split(\" \")\n",
    "    numbers = re.compile(r\"\\d+\")\n",
    "    new_text_list = []\n",
    "    for w in split:\n",
    "        if w == '@url':\n",
    "            new_text_list.append('@URL')\n",
    "            continue\n",
    "            \n",
    "        if w == '@mention':\n",
    "            new_text_list.append('@MENTION')\n",
    "            continue\n",
    "            \n",
    "        if numbers.match(w):\n",
    "            new_text_list.append('@NUMBER')\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            new_text_list.append(w)\n",
    "    \n",
    "    return \" \".join(new_text_list)\n",
    "\n",
    "def whitespace_replace_udf(tweet):\n",
    "    return strip_multiple_whitespaces(tweet)\n",
    "\n",
    "@udf(\"string\")\n",
    "def process_text(tweet):\n",
    "    tweet = my_remove_stopwords(tweet)\n",
    "    tweet = replacer(tweet)\n",
    "    tweet = ignore_stemmer(tweet)\n",
    "    tweet = further_replacer(tweet)\n",
    "    tweet = strip_multiple_whitespaces(tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "df = df.withColumn(\"cleaned_text\", process_text(\"entity_replaced\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T04:38:25.013600Z",
     "start_time": "2020-07-01T04:38:24.964342Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split    \n",
    "df = df.withColumn(\"split_text\", split(df.cleaned_text, \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T04:54:42.121911Z",
     "start_time": "2020-07-01T04:40:11.506594Z"
    }
   },
   "outputs": [],
   "source": [
    "df.write.option(\"maxRecordsPerFile\", 50000).json(\"cleaned_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features)\n",
    "tf = tf_vectorizer.fit_transform(pdf.cleaned_text.values)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pdf.split_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T18:01:37.637717Z",
     "start_time": "2020-05-30T17:48:07.458Z"
    }
   },
   "outputs": [],
   "source": [
    "# need to confirm it's right to do dict with full text but other things not with it \n",
    "dictionary = Dictionary(df.split_text.to_list())\n",
    "\n",
    "training_df = df[df.rt_indicator == 0]\n",
    "\n",
    "training_docs = list(training_df.split_text.apply(tuple).unique())\n",
    "training_corpus = [dictionary.doc2bow(tweet) for tweet in training_docs]\n",
    "\n",
    "full_docs = df.split_text.to_list()\n",
    "full_corpus = [dictionary.doc2bow(tweet) for tweet in full_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T18:01:37.640362Z",
     "start_time": "2020-05-30T17:48:09.182Z"
    }
   },
   "outputs": [],
   "source": [
    "len(training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T18:01:37.644958Z",
     "start_time": "2020-05-30T17:48:09.874Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import ldamulticore\n",
    "# main hyperparameter is number of topics, 10 may be too little, try 50 or 100 for this random sample dataset\n",
    "# for coronavirus themed tweets, we could do fewer topics \n",
    "\n",
    "# Set training parameters.\n",
    "# try different number of topics\n",
    "num_topics = 10\n",
    "chunksize = 2000 # number of documents passed to a core\n",
    "\n",
    "# use defaults for iterations and passes and see if modeling is good\n",
    "passes = 20 # number of passes through corpus\n",
    "iterations = 400 # could make 100 for coronavirus tweets, but could reduce for faster development iterations \n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = ldamulticore.LdaMulticore(\n",
    "    corpus=training_corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    eta='auto',\n",
    "    iterations=400,\n",
    "    num_topics=50,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T18:01:37.652692Z",
     "start_time": "2020-05-30T17:48:12.211Z"
    }
   },
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T18:01:37.760595Z",
     "start_time": "2020-05-30T17:48:12.627Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:40:43.246124Z",
     "start_time": "2020-05-22T15:40:43.205936Z"
    }
   },
   "outputs": [],
   "source": [
    "topics_dict = {}\n",
    "for t in range(len(top_topics)):\n",
    "    topics_dict[str(t)] = top_topics[t]\n",
    "print(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:16:11.070997Z",
     "start_time": "2020-05-22T06:06:05.894916Z"
    }
   },
   "outputs": [],
   "source": [
    "topics = []\n",
    "for i in range(len(full_corpus)):\n",
    "    topics.append(model.get_document_topics(full_corpus[i], minimum_probability=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:17:10.161063Z",
     "start_time": "2020-05-22T06:16:11.314658Z"
    }
   },
   "outputs": [],
   "source": [
    "probabilities = []\n",
    "for t in topics:\n",
    "    p = list(zip(*t))[1]\n",
    "    probabilities.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:17:19.102248Z",
     "start_time": "2020-05-22T06:17:10.383712Z"
    }
   },
   "outputs": [],
   "source": [
    "df['topics'] = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:17:19.362459Z",
     "start_time": "2020-05-22T06:17:19.120026Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "import math\n",
    "\n",
    "def calculate_perplexity(probability_list):\n",
    "    return math.exp(entropy(probability_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:17:52.866726Z",
     "start_time": "2020-05-22T06:17:19.393941Z"
    }
   },
   "outputs": [],
   "source": [
    "df['perplexity'] = df['topics'].apply(calculate_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:17:58.847528Z",
     "start_time": "2020-05-22T06:17:52.884313Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(df['perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:18:30.683282Z",
     "start_time": "2020-05-22T06:17:58.852996Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['top_topic'] = df['topics'].apply(np.argmax)\n",
    "df['top_topic_prob'] = df['topics'].apply(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:21:24.035690Z",
     "start_time": "2020-05-22T06:18:30.705124Z"
    }
   },
   "outputs": [],
   "source": [
    "# throw away tweets with perplexity too high\n",
    "unperplexed = df[df['perplexity'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:23:13.175548Z",
     "start_time": "2020-05-22T06:21:24.361977Z"
    }
   },
   "outputs": [],
   "source": [
    "unperplexed.to_csv(\"50_topic_model_round_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:39:07.771014Z",
     "start_time": "2020-05-22T06:23:13.209763Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "for i in range(50):\n",
    "    sample = unperplexed[unperplexed['top_topic'] == i]\n",
    "    ordered = sample.loc[sample.topics.apply(lambda x: x[i]).sort_values(ascending = False).index]\n",
    "    text_dict[i] = list(ordered.entity_replaced.unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:39:07.921231Z",
     "start_time": "2020-05-22T06:39:07.815771Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"50_topics_tweets_round_4.json\", \"w\") as f:\n",
    "    f.write(json.dumps(text_dict, indent = 2, ensure_ascii = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T04:12:37.397199Z",
     "start_time": "2020-05-19T04:01:04.553Z"
    }
   },
   "outputs": [],
   "source": [
    "perplexity_dict = {}\n",
    "perplexity_dict_tuples = {}\n",
    "nrows_dict = {}\n",
    "\n",
    "def make_float(l):\n",
    "    return [float(i) for i in l]\n",
    "        \n",
    "for i in range(1, 11):\n",
    "    key = \"[\" + str(i) + \",\"  + str(i + 1) + \")\"\n",
    "    ix = pdf[(pdf.perplexity >= i) & (pdf.perplexity < i+1)].index\n",
    "    sample = pdf.loc[ix, ['full_text', 'topics', 'perplexity']]\n",
    "    nrows = sample.shape[0]\n",
    "    sample = sample.sample(n=100)\n",
    "    sample['topics'] = sample['topics'].apply(make_float)\n",
    "    \n",
    "    nrows_dict[key] = nrows\n",
    "    \n",
    "    perplexity_dict[key] = {}\n",
    "    perplexity_dict[key][\"full_text\"] = sample.full_text.to_list()\n",
    "    perplexity_dict[key][\"topic_probabilities\"] = sample.topics.to_list()\n",
    "    perplexity_dict[key][\"perplexity\"] = sample.perplexity.to_list()\n",
    "    \n",
    "    perplexity_dict_tuples[key] = list(zip(sample.full_text.to_list(), sample.topics.to_list(), sample.perplexity.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T03:41:21.288779Z",
     "start_time": "2020-05-19T02:59:39.435Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"perplexity_sample.json\", \"w\") as f:\n",
    "    f.write(json.dumps(perplexity_dict, indent = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T03:41:21.292007Z",
     "start_time": "2020-05-19T02:59:40.198Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"perplexity_sample_tuple.json\", \"w\") as f:\n",
    "    f.write(json.dumps(perplexity_dict_tuples, indent = 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
