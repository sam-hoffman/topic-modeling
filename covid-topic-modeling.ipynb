{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:22:13.426736Z",
     "start_time": "2020-05-14T00:22:09.047875Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import preprocess_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T02:33:21.685168Z",
     "start_time": "2020-05-14T02:33:20.412134Z"
    }
   },
   "outputs": [],
   "source": [
    "# read with spark because of nested list column \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.parquet(\"long-parsed-tweets2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T02:34:08.017448Z",
     "start_time": "2020-05-14T02:33:23.293424Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.where(df.lang == \"en\").sample(0.001)\n",
    "training_df = df.where(~ df.full_text.like(\"RT @%\")).toPandas()\n",
    "full_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T02:53:04.992464Z",
     "start_time": "2020-05-14T02:53:04.982292Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_entities(row):\n",
    "    mentions = row['mentions']\n",
    "    urls = row['urls']\n",
    "    tweet = row['full_text']\n",
    "    \n",
    "    mentions = ['@' + m for m in mentions.split(' ')]\n",
    "    for m in mentions:\n",
    "        if len(m) > 0:\n",
    "            tweet = tweet.replace(m, '@MENTION')\n",
    "        \n",
    "    urls = urls.split(' ')\n",
    "    for u in urls:\n",
    "        if len(u) > 0:\n",
    "            tweet = tweet.replace(u, '@URL')\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T02:53:07.702029Z",
     "start_time": "2020-05-14T02:53:07.563803Z"
    }
   },
   "outputs": [],
   "source": [
    "training_df['replaced_text'] = training_df.apply(replace_entities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:10.364752Z",
     "start_time": "2020-05-14T03:13:08.032809Z"
    }
   },
   "outputs": [],
   "source": [
    "from demoji import replace\n",
    "import re\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces, remove_stopwords\n",
    "from gensim.utils import to_unicode\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "STOPWORDS = STOPWORDS.union(stopwords.words('english')).union(set('&amp;'))\n",
    "def my_remove_stopwords(s):\n",
    "    s = to_unicode(s)\n",
    "    s = s.lower()\n",
    "    return \" \".join(w for w in s.split() if w not in STOPWORDS)\n",
    "\n",
    "def regexer(string):\n",
    "    http = re.compile(r'https?://\\S+')\n",
    "    string = http.sub('@URL', string)\n",
    "    string = string.replace(\"-\", \" \")\n",
    "    string = string.replace(\"RT \", \"\")\n",
    "    handles_and_hashtags = re.compile(r\"[^\\w\\d#@\\s]+\")\n",
    "    string =  handles_and_hashtags.sub('', string)\n",
    "    return string\n",
    "\n",
    "custom_filters = [\n",
    "                  replace,\n",
    "                  strip_multiple_whitespaces,\n",
    "                  regexer,\n",
    "                  my_remove_stopwords,\n",
    "                  SnowballStemmer(\"english\").stem, \n",
    "                 ]\n",
    "training_df['text'] = training_df.replaced_text.apply(preprocess_string, filters=custom_filters)\n",
    "\n",
    "def further_replacer(text_list):\n",
    "    numbers = re.compile(r\"\\d+\")\n",
    "    new_text_list = []\n",
    "    for w in text_list:\n",
    "        if w == '@url':\n",
    "            new_text_list.append('@URL')\n",
    "            continue\n",
    "            \n",
    "        if w == '@mention':\n",
    "            new_text_list.append('@MENTION')\n",
    "            continue\n",
    "            \n",
    "        if numbers.match(w):\n",
    "            new_text_list.append('@NUMBER')\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            new_text_list.append(w)\n",
    "    \n",
    "    return new_text_list\n",
    "\n",
    "training_df['text'] = training_df.text.apply(further_replacer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:45.842028Z",
     "start_time": "2020-05-14T03:13:45.363087Z"
    }
   },
   "outputs": [],
   "source": [
    "training_df[\"created_at\"] = pd.to_datetime(training_df.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:34.816939Z",
     "start_time": "2020-05-14T03:13:34.810687Z"
    }
   },
   "outputs": [],
   "source": [
    "training_docs = training_df.text.to_list()\n",
    "training_dictionary = Dictionary(docs)\n",
    "training_corpus = [training_dictionary.doc2bow(tweet) for tweet in training_docs]\n",
    "\n",
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:48.355963Z",
     "start_time": "2020-05-14T03:13:48.292679Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to make topics have fewer words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:52.917030Z",
     "start_time": "2020-05-14T03:13:50.629574Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import ldamulticore\n",
    "# main hyperparameter is number of topics, 10 may be too little, try 50 or 100 for this random sample dataset\n",
    "# for coronavirus themed tweets, we could do fewer topics \n",
    "\n",
    "# Set training parameters.\n",
    "# try different number of topics\n",
    "num_topics = 10\n",
    "chunksize = 2000 # number of documents passed to a core\n",
    "\n",
    "# use defaults for iterations and passes and see if modeling is good\n",
    "passes = 20 # number of passes through corpus\n",
    "iterations = 400 # could make 100 for coronavirus tweets, but could reduce for faster development iterations \n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = ldamulticore.LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    eta='auto',\n",
    "    iterations=400,\n",
    "    num_topics=5,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:52.969346Z",
     "start_time": "2020-05-14T03:13:52.921619Z"
    }
   },
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:55.192772Z",
     "start_time": "2020-05-14T03:13:54.332570Z"
    }
   },
   "outputs": [],
   "source": [
    "# how 2 do this in spark??\n",
    "# add corpus as column?\n",
    "topics = []\n",
    "for i in range(len(corpus)):\n",
    "    topics.append(model.get_document_topics(corpus[i], minimum_probability=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:11:02.301380Z",
     "start_time": "2020-05-13T04:11:00.102788Z"
    }
   },
   "outputs": [],
   "source": [
    "training_df['topics'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:11:03.441587Z",
     "start_time": "2020-05-13T04:11:02.308744Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_df = pd.DataFrame(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:11:06.319798Z",
     "start_time": "2020-05-13T04:11:03.446059Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_df = topic_df.applymap(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:11:16.408452Z",
     "start_time": "2020-05-13T04:11:06.324435Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([pdf.reset_index(drop=True), topic_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:12:12.815842Z",
     "start_time": "2020-05-13T04:11:16.418450Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"5_topic_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:12:22.825780Z",
     "start_time": "2020-05-13T04:12:12.832001Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[5] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6fd01644c3c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msmall_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msmall_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmall_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtext_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/covid-tweets/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/covid-tweets/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         )\n\u001b[1;32m   1555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/covid-tweets/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loc\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[5] not in index'"
     ]
    }
   ],
   "source": [
    "text_dict = {}\n",
    "for i in range(10):\n",
    "    small_df = df[['full_text', i]]\n",
    "    small_df = small_df.sort_values(i, ascending=False)\n",
    "    text_dict[i] = list(small_df.full_text.unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:11:54.295141Z",
     "start_time": "2020-05-14T00:11:54.024004Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"5_topics_tweets.json\", \"w\") as f:\n",
    "    f.write(json.dumps(text_dict, indent = 2, ensure_ascii = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
