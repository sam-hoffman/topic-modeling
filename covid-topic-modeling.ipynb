{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T03:29:37.214539Z",
     "start_time": "2020-05-15T03:29:34.898998Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import preprocess_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T03:59:30.887261Z",
     "start_time": "2020-05-15T03:59:30.607009Z"
    }
   },
   "outputs": [],
   "source": [
    "# read with spark because of nested list column \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.parquet(\"long-parsed-tweets2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T03:59:51.574522Z",
     "start_time": "2020-05-15T03:59:50.295111Z"
    }
   },
   "outputs": [],
   "source": [
    "df.where(df.lang == \"en\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T04:09:35.282308Z",
     "start_time": "2020-05-15T04:08:43.937620Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.where(df.lang == \"en\").sample(0.05)\n",
    "training_df = df.where(~ df.full_text.like(\"RT @%\")).sample(0.05).toPandas()\n",
    "full_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T04:09:35.297266Z",
     "start_time": "2020-05-15T04:09:35.288097Z"
    }
   },
   "outputs": [],
   "source": [
    "import datefinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T04:09:35.313666Z",
     "start_time": "2020-05-15T04:09:35.307693Z"
    }
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T04:11:06.143414Z",
     "start_time": "2020-05-15T04:11:06.133485Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def replace_entities(row):\n",
    "    mentions = row['mentions']\n",
    "    urls = row['urls']\n",
    "    tweet = row['full_text']\n",
    "    \n",
    "    mentions = ['@' + m for m in mentions.split(' ')]\n",
    "    for m in mentions:\n",
    "        if len(m) > 0:\n",
    "            tweet = tweet.replace(m, '@MENTION')\n",
    "        \n",
    "    urls = urls.split(' ')\n",
    "    for u in urls:\n",
    "        if len(u) > 0:\n",
    "            tweet = tweet.replace(u, '@URL')\n",
    "    http = re.compile(r'https?://\\S+')\n",
    "    tweet = http.sub('@URL', tweet)\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T04:11:06.523737Z",
     "start_time": "2020-05-15T04:11:06.486429Z"
    }
   },
   "outputs": [],
   "source": [
    "training_df['entity_replaced'] = training_df.apply(replace_entities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T04:11:08.187564Z",
     "start_time": "2020-05-15T04:11:08.179769Z"
    }
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    " \n",
    "def remove_dates(tweet): \n",
    "    try:\n",
    "        parsed = parse(tweet, fuzzy_with_tokens=True) \n",
    "        text = ' '.join(parsed[1])\n",
    "    except:\n",
    "        text = tweet\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T04:11:10.182015Z",
     "start_time": "2020-05-15T04:11:10.023441Z"
    }
   },
   "outputs": [],
   "source": [
    "training_df['entity_and_date_replaced'] = training_df.entity_replaced.apply(remove_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T04:11:12.494554Z",
     "start_time": "2020-05-15T04:11:12.461348Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_df[training_df['entity_replaced'] != training_df['entity_and_date_replaced']][['entity_replaced', 'entity_and_date_replaced']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T04:11:16.229156Z",
     "start_time": "2020-05-15T04:11:16.218942Z"
    }
   },
   "outputs": [],
   "source": [
    "training_df.loc[0, 'full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T04:11:18.254363Z",
     "start_time": "2020-05-15T04:11:18.244181Z"
    }
   },
   "outputs": [],
   "source": [
    "parse(training_df.loc[0, 'entity_replaced'], fuzzy_with_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T04:11:22.527298Z",
     "start_time": "2020-05-15T04:11:22.471273Z"
    }
   },
   "outputs": [],
   "source": [
    "for m in datefinder.find_dates(training_df.loc[0, 'entity_replaced']):\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: figure out how to ignore dates that are parsed out of covid-19 without removing covid-19 from the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:10.364752Z",
     "start_time": "2020-05-14T03:13:08.032809Z"
    }
   },
   "outputs": [],
   "source": [
    "from demoji import replace\n",
    "import re\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces, remove_stopwords\n",
    "from gensim.utils import to_unicode\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "STOPWORDS = STOPWORDS.union(stopwords.words('english')).union(set('&amp;'))\n",
    "def my_remove_stopwords(s):\n",
    "    s = to_unicode(s)\n",
    "    s = s.lower()\n",
    "    return \" \".join(w for w in s.split() if w not in STOPWORDS)\n",
    "\n",
    "def regexer(string):\n",
    "    string = string.replace(\"-\", \" \")\n",
    "    string = string.replace(\"RT \", \"\")\n",
    "    handles_and_hashtags = re.compile(r\"[^\\w\\d#@\\s]+\")\n",
    "    string =  handles_and_hashtags.sub('', string)\n",
    "    return string\n",
    "\n",
    "custom_filters = [\n",
    "                  replace,\n",
    "                  strip_multiple_whitespaces,\n",
    "                  regexer,\n",
    "                  my_remove_stopwords,\n",
    "                  SnowballStemmer(\"english\").stem, \n",
    "                 ]\n",
    "training_df['text'] = training_df.replaced_text.apply(preprocess_string, filters=custom_filters)\n",
    "\n",
    "def further_replacer(text_list):\n",
    "    numbers = re.compile(r\"\\d+\")\n",
    "    new_text_list = []\n",
    "    for w in text_list:\n",
    "        if w == '@url':\n",
    "            new_text_list.append('@URL')\n",
    "            continue\n",
    "            \n",
    "        if w == '@mention':\n",
    "            new_text_list.append('@MENTION')\n",
    "            continue\n",
    "            \n",
    "        if numbers.match(w):\n",
    "            new_text_list.append('@NUMBER')\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            new_text_list.append(w)\n",
    "    \n",
    "    return new_text_list\n",
    "\n",
    "training_df['text'] = training_df.text.apply(further_replacer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:45.842028Z",
     "start_time": "2020-05-14T03:13:45.363087Z"
    }
   },
   "outputs": [],
   "source": [
    "training_df[\"created_at\"] = pd.to_datetime(training_df.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:34.816939Z",
     "start_time": "2020-05-14T03:13:34.810687Z"
    }
   },
   "outputs": [],
   "source": [
    "training_docs = training_df.text.to_list()\n",
    "training_dictionary = Dictionary(docs)\n",
    "training_corpus = [training_dictionary.doc2bow(tweet) for tweet in training_docs]\n",
    "\n",
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:48.355963Z",
     "start_time": "2020-05-14T03:13:48.292679Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to make topics have fewer words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:52.917030Z",
     "start_time": "2020-05-14T03:13:50.629574Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import ldamulticore\n",
    "# main hyperparameter is number of topics, 10 may be too little, try 50 or 100 for this random sample dataset\n",
    "# for coronavirus themed tweets, we could do fewer topics \n",
    "\n",
    "# Set training parameters.\n",
    "# try different number of topics\n",
    "num_topics = 10\n",
    "chunksize = 2000 # number of documents passed to a core\n",
    "\n",
    "# use defaults for iterations and passes and see if modeling is good\n",
    "passes = 20 # number of passes through corpus\n",
    "iterations = 400 # could make 100 for coronavirus tweets, but could reduce for faster development iterations \n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = ldamulticore.LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    eta='auto',\n",
    "    iterations=400,\n",
    "    num_topics=5,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:52.969346Z",
     "start_time": "2020-05-14T03:13:52.921619Z"
    }
   },
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:13:55.192772Z",
     "start_time": "2020-05-14T03:13:54.332570Z"
    }
   },
   "outputs": [],
   "source": [
    "# how 2 do this in spark??\n",
    "# add corpus as column?\n",
    "topics = []\n",
    "for i in range(len(corpus)):\n",
    "    topics.append(model.get_document_topics(corpus[i], minimum_probability=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:11:02.301380Z",
     "start_time": "2020-05-13T04:11:00.102788Z"
    }
   },
   "outputs": [],
   "source": [
    "training_df['topics'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:11:03.441587Z",
     "start_time": "2020-05-13T04:11:02.308744Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_df = pd.DataFrame(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:11:06.319798Z",
     "start_time": "2020-05-13T04:11:03.446059Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_df = topic_df.applymap(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:11:16.408452Z",
     "start_time": "2020-05-13T04:11:06.324435Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([pdf.reset_index(drop=True), topic_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:12:12.815842Z",
     "start_time": "2020-05-13T04:11:16.418450Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"5_topic_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:12:22.825780Z",
     "start_time": "2020-05-13T04:12:12.832001Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "for i in range(10):\n",
    "    small_df = df[['full_text', i]]\n",
    "    small_df = small_df.sort_values(i, ascending=False)\n",
    "    text_dict[i] = list(small_df.full_text.unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:11:54.295141Z",
     "start_time": "2020-05-14T00:11:54.024004Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"5_topics_tweets.json\", \"w\") as f:\n",
    "    f.write(json.dumps(text_dict, indent = 2, ensure_ascii = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
